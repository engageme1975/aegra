# Aegra + Agent Chat UI Integration Guide

## Overview

This guide shows how to connect the **Agent Chat UI** (Next.js frontend) with **Aegra** (FastAPI backend) for a complete agent chat application.

**Status**: ✅ Ready to run

---

## Prerequisites

- Python 3.11+
- Node.js 18+
- Docker & Docker Compose (for PostgreSQL)
- `uv` Python package manager
- `pnpm` Node package manager

---

## Quick Start

### 1️⃣ Start Aegra Backend

```bash
# Navigate to aegra directory
cd /home/aegra

# Install dependencies (if not done)
uv install

# Start PostgreSQL database
docker compose up postgres -d

# Run migrations
python3 scripts/migrate.py upgrade

# Start the backend server
uv run uvicorn src.agent_server.main:app --reload --host 0.0.0.0 --port 8000
```

**Expected output:**
```
Uvicorn running on http://0.0.0.0:8000
Press CTRL+C to quit
```

**Test the backend:**
```bash
curl http://localhost:8000/health
# Should return: {"status": "ok"}
```

### 2️⃣ Start Agent Chat UI

```bash
# In a new terminal, navigate to agent-chat-ui
cd /home/agent-chat-ui

# Install dependencies (if not done)
pnpm install

# Start development server
pnpm dev
```

**Expected output:**
```
> ready - started server on 0.0.0.0:3000, url: http://localhost:3000
```

### 3️⃣ Access the Chat Interface

Open your browser and navigate to:
```
http://localhost:3000
```

You should see the Agent Chat UI setup form.

---

## Configuration

### Aegra Backend (.env)

The backend uses `noop` authentication by default (no auth required). To enable custom authentication:

```bash
# Set authentication type
export AUTH_TYPE=noop    # Default: no auth required
# or
export AUTH_TYPE=custom  # Custom authentication (edit auth.py)
```

### Agent Chat UI (.env)

File: `/home/agent-chat-ui/.env`

```env
# Backend API URL
NEXT_PUBLIC_API_URL=http://localhost:8000

# Assistant/Graph ID to use
NEXT_PUBLIC_ASSISTANT_ID=agent

# Optional: LangSmith API key (only for production deployments)
LANGSMITH_API_KEY=
```

---

## API Endpoints

### OpenAI-Compatible Endpoints

#### 1. List Available Models (Graphs)

```bash
curl http://localhost:8000/v1/models
```

**Response:**
```json
{
  "object": "list",
  "data": [
    {
      "id": "agent",
      "object": "model",
      "created": 1700000000,
      "owned_by": "aegra"
    },
    {
      "id": "agent_hitl",
      "object": "model",
      "created": 1700000000,
      "owned_by": "aegra"
    }
  ]
}
```

#### 2. Chat Completions (Non-Streaming)

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "agent",
    "messages": [
      {"role": "user", "content": "Hello"}
    ],
    "thread_id": "thread-123"
  }'
```

**Response:**
```json
{
  "id": "chatcmpl-...",
  "object": "chat.completion",
  "created": 1700000000,
  "model": "agent",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 20,
    "total_tokens": 30
  }
}
```

#### 3. Chat Completions (Streaming)

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "agent",
    "messages": [
      {"role": "user", "content": "Tell me a story"}
    ],
    "stream": true,
    "thread_id": "thread-123"
  }'
```

**Response (SSE stream):**
```
data: {"id":"chatcmpl-...","object":"chat.completion.chunk","model":"agent","choices":[{"index":0,"delta":{"role":"assistant","content":"Once"},"finish_reason":null}]}

data: {"id":"chatcmpl-...","object":"chat.completion.chunk","model":"agent","choices":[{"index":0,"delta":{"role":"assistant","content":" upon"},"finish_reason":null}]}

data: [DONE]
```

---

## Features

### Multi-Agent Support

Select any agent from the dropdown when configuring the chat UI:

```
Available agents from aegra.json:
- agent (react_agent)
- agent_hitl (react_agent_hitl)
- subgraph_agent
- subgraph_hitl_agent
- uk_housing (if configured)
```

### Thread Management

Each conversation is associated with a thread ID for memory persistence:

```python
# Browser automatically generates thread ID
# or provide custom one: ?threadId=my-thread-id

# All messages in the same thread are persisted
# and available in subsequent requests
```

### Streaming Support

Real-time message streaming from agents:

```javascript
// Agent Chat UI automatically handles streaming
// Messages appear as they're generated by the LLM
```

---

## Troubleshooting

### ❌ "Failed to connect to LangGraph server"

**Problem:** Agent Chat UI cannot reach Aegra backend

**Solutions:**
1. Verify Aegra is running on `http://localhost:8000`
2. Check firewall/network settings
3. Verify `.env` file has correct `NEXT_PUBLIC_API_URL`

```bash
# Test backend health
curl http://localhost:8000/health

# Check environment variables
echo $NEXT_PUBLIC_API_URL  # Should be http://localhost:8000
```

### ❌ "Model/Graph not found"

**Problem:** Selected model doesn't exist in Aegra

**Solutions:**
1. Check available models:
   ```bash
   curl http://localhost:8000/v1/models
   ```
2. Verify `aegra.json` has the correct graph defined
3. Verify the graph file exists at the specified path

### ❌ "Port 8000 already in use"

**Problem:** Aegra can't start because port is in use

**Solutions:**
```bash
# Find what's using port 8000
lsof -i :8000

# Kill the process
kill -9 <PID>

# Or use a different port
uv run uvicorn src.agent_server.main:app --port 8001
```

### ❌ "No threads showing in Agent Chat UI"

**Problem:** Agent Chat UI isn't creating/loading threads

**Solutions:**
1. Verify `NEXT_PUBLIC_API_URL` in `.env`
2. Check browser console for errors
3. Test the threads API:
   ```bash
   curl http://localhost:8000/threads
   ```

---

## Available Graphs

The graphs are configured in `/home/aegra/aegra.json`:

```json
{
  "graphs": {
    "agent": "./graphs/react_agent/graph.py:graph",
    "agent_hitl": "./graphs/react_agent_hitl/graph.py:graph",
    "subgraph_agent": "./graphs/subgraph_agent/graph.py:graph",
    "subgraph_hitl_agent": "./graphs/subgraph_hitl_agent/graph.py:graph",
    "uk_housing": "./graphs/uk_housing_agent/graph.py:graph"
  }
}
```

Each graph ID is available as a model in the OpenAI-compatible API.

---

## Deployment (Production)

### Using Docker Compose

```bash
# Start all services (backend + database + frontend)
docker compose up -d

# View logs
docker compose logs -f

# Stop all services
docker compose down
```

### Environment Variables for Production

```env
# Aegra
AUTH_TYPE=custom          # Enable authentication
DATABASE_URL=postgresql://user:pass@db:5432/aegra
PORT=8000

# Agent Chat UI
NEXT_PUBLIC_API_URL=https://your-domain.com
NEXT_PUBLIC_ASSISTANT_ID=agent
LANGSMITH_API_KEY=lsv2_...
```

---

## Advanced Configuration

### Custom Authentication

Edit `/home/aegra/auth.py` to integrate with your auth service:

```python
@auth.authenticate
async def authenticate(headers: dict[str, str]) -> Auth.types.MinimalUserDict:
    """Custom authentication handler"""
    token = headers.get("authorization", "").replace("Bearer ", "")
    
    # Validate token with your auth service
    user = validate_token(token)
    
    return {
        "identity": user.id,
        "display_name": user.name,
        "permissions": user.permissions,
        "org_id": user.org_id,
        "is_authenticated": True,
    }
```

Then set `AUTH_TYPE=custom` and restart Aegra.

### Adding New Graphs

1. Create your graph in `/home/aegra/graphs/my_graph/graph.py`
2. Add to `aegra.json`:
   ```json
   {
     "graphs": {
       "my_agent": "./graphs/my_graph/graph.py:graph"
     }
   }
   ```
3. Restart Aegra - the new graph will appear in the models list

---

## Testing

### Run Integration Test

```bash
bash /home/test_integration.sh
```

This script verifies:
- ✅ Aegra backend is running
- ✅ Models endpoint responds
- ✅ Chat completion endpoint works
- ✅ Agent Chat UI is accessible

### Manual Testing

```bash
# 1. Start backend
cd /home/aegra
uv run uvicorn src.agent_server.main:app --reload

# 2. In new terminal, test endpoints
curl http://localhost:8000/v1/models

# 3. Test chat completion
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "agent",
    "messages": [{"role": "user", "content": "Hello"}]
  }'

# 4. Start frontend
cd /home/agent-chat-ui
pnpm dev

# 5. Open http://localhost:3000
```

---

## Architecture

```
┌─────────────────────────────────────┐
│   Agent Chat UI (Next.js)           │
│   http://localhost:3000             │
└────────────┬────────────────────────┘
             │ HTTP/WebSocket
             │ OpenAI-compatible API
             ↓
┌─────────────────────────────────────┐
│   Aegra Backend (FastAPI)           │
│   http://localhost:8000             │
│  ┌──────────────────────────────┐   │
│  │ OpenAI-compatible Endpoints  │   │
│  │ /v1/models                   │   │
│  │ /v1/chat/completions         │   │
│  └──────────────────────────────┘   │
└────────────┬────────────────────────┘
             │ LangGraph SDK
             ↓
┌─────────────────────────────────────┐
│   LangGraph Agents                  │
│   - agent (React Agent)             │
│   - agent_hitl (HITL Agent)         │
│   - custom agents...                │
└────────────┬────────────────────────┘
             │ State persistence
             ↓
┌─────────────────────────────────────┐
│   PostgreSQL Database               │
│   - Agent state checkpoints         │
│   - Thread history                  │
│   - Metadata                        │
└─────────────────────────────────────┘
```

---

## Support

For issues or questions:

1. Check troubleshooting section above
2. Review Aegra logs: `docker compose logs -f aegra`
3. Check browser console for frontend errors
4. Review Agent Chat UI docs: `/home/agent-chat-ui/README.md`
5. Review Aegra docs: `/home/aegra/CLAUDE.md`

---

## Next Steps

1. ✅ Start Aegra backend
2. ✅ Start Agent Chat UI
3. ✅ Configure API URL and Assistant ID
4. ✅ Select an agent and start chatting
5. Customize agents for your use case
6. Deploy to production using Docker

---

**Last Updated:** November 17, 2025  
**Status:** ✅ Production Ready
